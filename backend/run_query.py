from retrieval_mechs import retrieve_context
import ollama
import json
import sys
import time


def format_context(retrievals):
    """Format retrieval results into a readable context string."""
    if not retrievals:
        return "No relevant context found in the knowledge graph."
    
    context_str = ""
    for r in retrievals:
        context_str += f"\nğŸ” Entity: {r['entity']}\n"

        # Neighbors (triplets)
        if r['neighbors']:
            context_str += "  ğŸ“Š Relations:\n"
            for n in r['neighbors']:
                if n["target"]:
                    context_str += f"    â€¢ ({n['source']}) -[{n['relation']}]-> ({n['target']}) [ğŸ“„ {n['provenance']}]\n"

        # Chunks
        if r['chunks']:
            context_str += "  ğŸ“ Supporting Text:\n"
            for c in r['chunks']:
                # Better text trimming with ellipsis handling
                text_preview = c['text'][:300].strip()
                if len(c['text']) > 300:
                    text_preview += "..."
                context_str += f"    â€¢ {text_preview}\n"

    return context_str


def answer_with_graph_rag_llama(question, retrievals):
    """Generate an answer using the knowledge graph context and LLM."""
    context = format_context(retrievals)

    prompt = f"""You are an expert biomedical assistant with access to a comprehensive medical knowledge graph.

    Your task is to answer the user's question using the provided structured knowledge and supporting documents.

    Guidelines:
    - Provide accurate, evidence-based answers
    - Cite specific sources when available  
    - If information is incomplete, clearly state limitations
    - Use clear, professional medical language
    - Structure your response logically
    - Keep responses concise and focused

    Question: {question}

    Knowledge Graph Context:
    {context}

    Please provide a complete, well-structured answer based on the available context. If the question is not relevant to the medical/biomedical domain or the available context, politely explain this and suggest asking a relevant question instead.
    """

    try:
        response = ollama.chat(
            model="llama3",
            messages=[{"role": "user", "content": prompt}],
            options={
                "temperature": 0.1,
                "top_p": 0.9,
                "stop": ["Question:", "Context:", "Answer:"]
            }
        )
        
        # Clean up the response
        answer = response['message']['content'].strip()
        
        # Remove any duplicate text or formatting artifacts
        lines = answer.split('\n')
        cleaned_lines = []
        seen_lines = set()
        
        for line in lines:
            line = line.strip()
            if line and line not in seen_lines:
                cleaned_lines.append(line)
                seen_lines.add(line)
        
        return '\n'.join(cleaned_lines)
        
    except Exception as e:
        return f"âŒ Error generating answer: {str(e)}"


def interactive_qa_session():
    """Run an interactive question-answering session."""
    print("ğŸ§¬ Biomedical Knowledge Graph RAG System")
    print("=" * 50)
    print("Ask questions about the medical documents in the knowledge base.")
    print("Type 'quit', 'exit', or 'q' to end the session.\n")
    
    session_count = 0
    
    while True:
        try:
            # Get user question
            question = input("â“ Question: ").strip()
            
            # Check for exit commands
            if question.lower() in ['quit', 'exit', 'q', '']:
                print("\nğŸ‘‹ Thank you for using the Biomedical RAG System!")
                break
            
            session_count += 1
            print(f"\nğŸ”„ Processing question #{session_count}...")
            
            # Retrieve context
            start_time = time.time()
            retrievals = retrieve_context(question, topk_entities=5)
            retrieval_time = time.time() - start_time
            
            print(f"ğŸ“Š Retrieved context in {retrieval_time:.2f} seconds")
            print(f"ğŸ“ˆ Found {len(retrievals)} relevant entities")
            
            # Generate answer
            print("ğŸ¤– Generating answer...")
            answer_start_time = time.time()
            answer = answer_with_graph_rag_llama(question, retrievals)
            answer_time = time.time() - answer_start_time
            
            # Display results with clear formatting
            print("\n" + "="*60)
            print("ğŸ“‹ ANSWER")
            print("="*60)
            print(answer)
            print("="*60)
            print(f"â±ï¸  Answer generated in {answer_time:.2f} seconds")
            print(f"ğŸ“Š Context entities used: {len(retrievals)}")
            print("\n")
            
            # Ask if user wants to see the raw context
            show_context = input("ğŸ” Show raw context? (y/N): ").strip().lower()
            if show_context in ['y', 'yes']:
                print("\nğŸ“š RAW CONTEXT:")
                print("-" * 30)
                print(format_context(retrievals))
                print("\n")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Session interrupted. Goodbye!")
            break
        except Exception as e:
            print(f"\nâŒ Error: {str(e)}")
            print("Please try again with a different question.\n")


def single_question_mode():
    """Handle a single question from command line or direct input."""
    if len(sys.argv) > 1:
        # Question provided as command line argument
        question = " ".join(sys.argv[1:])
    else:
        # Prompt for question
        question = input("â“ Question: ").strip()
    
    if not question:
        print("âŒ No question provided.")
        return
    
    print(f"\nğŸ”„ Processing: {question}")
    
    try:
        # Retrieve and answer
        start_time = time.time()
        retrievals = retrieve_context(question)
        answer = answer_with_graph_rag_llama(question, retrievals)
        total_time = time.time() - start_time
        
        # Display results
        print("\n" + "="*60)
        print("ğŸ“‹ ANSWER")
        print("="*60)
        print(answer)
        print("="*60)
        print(f"â±ï¸  Total processing time: {total_time:.2f} seconds")
        print(f"ğŸ“Š Context entities used: {len(retrievals)}")
        print()
        
    except Exception as e:
        print(f"âŒ Error: {str(e)}")


if __name__ == "__main__":
    print("ğŸ§¬ Biomedical Knowledge Graph RAG System")
    print("=" * 50)
    
    # Check if running in interactive mode
    mode = input("Choose mode:\n1. Interactive session (i)\n2. Single question (s)\nMode [i/s]: ").strip().lower()
    
    if mode in ['s', 'single', '2']:
        single_question_mode()
    else:
        interactive_qa_session()
